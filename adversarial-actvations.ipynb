{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to experiment with image classifiers performance on adversarial examples.\n",
    "\n",
    "More specifically, the goal is to investigate how activation strengths compare between test examples and adversarially perturbed test examples.\n",
    "\n",
    "While attempting to set up this notebook, I recalled/learned for the first time that there is an entire field called **attributions** which attempts to answer the question \"What caused this neural net to give this output?\"\n",
    "(As a reminder, see [Building Blocks of Interpretability](filter_zero_activations) for a discussion of how attribution and feature visualization are complementary techniques for interpreting neural nets.)\n",
    "\n",
    "Different attribution techniques exist to answer different sharpenings of this question such as\n",
    "1. Which pixel of an image most strongly impacted the classification result?\n",
    "2. Which neurons in layer X most strongly impacted the classification result?\n",
    "3. Many, many other permutations of similar questions.\n",
    "\n",
    "The [Captum library](https://captum.ai) is an implementation of several types of attribution techniques for PyTorch.\n",
    "See their [algorithm page](https://captum.ai/docs/algorithms) for a list of supported algorithms and a short description of how each one works.\n",
    "\n",
    "Looking at activation strengths of neurons in specific layers when a net is applied to an adversarial example seems to be in the spirit of other, more complex, attribution methodologies.\n",
    "\n",
    "However, I was not able to find any papers which made use of neuronal activations specifically when doing an attribution analysis.\n",
    "\n",
    "Bear in mind that I did not look very closely as I mostly wanted to get my hands dirty instead of reading what other people have done.\n",
    "\n",
    "I would not be surprised if comparing activations has conceptual weaknesses as an attribution strategy when compared to other approaches, which tend to rely on integration versus simply studying individual activations.\n",
    "\n",
    "Regardless, for the sake of curiosity and testing conjectures made in our last meeting, it seemed fun and valuable to play around with.\n",
    "\n",
    "## Method\n",
    "I have implemented very basic plotting and statistical analysis for activation strenghts.\n",
    "The `torchvision.models.feature_extraction` module is used to extract activations from models.\n",
    "\n",
    "All adversarial examples are generated using fast gradient sign method (FGSM) applied to examples from ImageNet.\n",
    "FGSM is easy to implement and quick to calculate, but it is only one of many possible methods of generating adversarial examples which have been studied.\n",
    "\n",
    "I have only examined the activations of the AlexNet model, for a few test examples.\n",
    "Other models and examples might give different results.\n",
    "\n",
    "## Source Code\n",
    "Source code for this notebook is in large part based on [Adversarial_Attacks.ipynb](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb#scrollTo=RU-Ng1nP0Y6p), written by Philip Lippe from the University of Amsterdam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up common libraries.\n",
    "\n",
    "Uncomment the line `pl.seed_everything(42)` to make the results deterministic.\n",
    "As is, images will be pulled randomly from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np \n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial10\"\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to turn make results deterministic\n",
    "# pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "import zipfile\n",
    "# Github url where a pre-processed ImageNet data set is located; this data set was prepared by Philip Lippe\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
    "# Files to download\n",
    "pretrained_files = [(DATASET_PATH, \"TinyImageNet.zip\"), (CHECKPOINT_PATH, \"patches.zip\")]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for dir_name, file_name in pretrained_files:\n",
    "    file_path = os.path.join(dir_name, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
    "        if file_name.endswith(\".zip\"):\n",
    "            print(\"Unzipping file...\")\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(file_path.rsplit(\"/\",1)[0])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a ConvNet.\n",
    "The AlexNet model is used by default here, but this can be changed to any of the models included in the Torchvision library, such as various ResNet implementations.\n",
    "\n",
    "**Caution**: if you change the model used here, you might need to do some pre-processing to the data set to get it in the format which the model expects.\n",
    "It might be easier to simply begin from the original ImageNet database and then check PyTorch documentation for guidance about the format required for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN architecture pretrained on ImageNet\n",
    "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
    "# Change 'alexnet' to any other model included in torchvision, taking into account the above caution\n",
    "pretrained_model = torchvision.models.alexnet(pretrained=True)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "# No gradients needed for the network\n",
    "pretrained_model.eval()\n",
    "for p in pretrained_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data loader. \n",
    "The data loader will:\n",
    "- Normalize the ImageNet data\n",
    "- Handle loading the files\n",
    "- Handle loading labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Std from ImageNet\n",
    "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
    "# No resizing and center crop necessary as images are already preprocessed.\n",
    "plain_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN,\n",
    "                         std=NORM_STD)\n",
    "])\n",
    "\n",
    "# Load dataset and create data loader\n",
    "imagenet_path = os.path.join(\"../data\", \"TinyImageNet/\")\n",
    "assert os.path.isdir(imagenet_path), f\"Could not find the ImageNet dataset at expected path \\\"{imagenet_path}\\\". \" + \\\n",
    "                                     f\"Please make sure to have downloaded the ImageNet dataset here, or change the variable.\"\n",
    "dataset = torchvision.datasets.ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
    "data_loader = data.DataLoader(dataset, batch_size=32, shuffle=True, drop_last=False, num_workers=2)\n",
    "\n",
    "# Load label names to interpret the label numbers 0 to 999\n",
    "with open(os.path.join(imagenet_path, \"label_list.json\"), \"r\") as f:\n",
    "    label_names = json.load(f)\n",
    "    \n",
    "def get_label_index(lab_str):\n",
    "    assert lab_str in label_names, f\"Label \\\"{lab_str}\\\" not found. Check the spelling of the class.\"\n",
    "    return label_names.index(lab_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an initial validation on the model.\n",
    "The dataset has been clipped for performance reasons.\n",
    "Change the value of `max_validations` to 0 to use the full dataset.\n",
    "\n",
    "First, define an `eval_model` function to perform validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(dataset_loader, max_validations = 1000, img_func=None):\n",
    "    tp, tp_5, counter = 0., 0., 0.\n",
    "    iterator = tqdm(dataset_loader, desc=\"Validating...\")\n",
    "    for imgs, labels in iterator:\n",
    "        # Break out of loop if we have exceeded max_validations\n",
    "        if max_validations != 0 and counter > max_validations:\n",
    "            iterator.close()\n",
    "            break\n",
    "        \n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if img_func is not None:\n",
    "            imgs = img_func(imgs, labels) \n",
    "        with torch.no_grad():\n",
    "            preds = pretrained_model(imgs)\n",
    "        tp += (preds.argmax(dim=-1) == labels).sum()\n",
    "        tp_5 += (preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
    "        counter += preds.shape[0]\n",
    "    acc = tp.float().item()/counter\n",
    "    top5 = tp_5.float().item()/counter\n",
    "    print(f\"Top-1 error: {(100.0 * (1 - acc)):4.2f}%\")\n",
    "    print(f\"Top-5 error: {(100.0 * (1 - top5)):4.2f}%\")\n",
    "    print(f\"Validations performed: {counter}\")\n",
    "    return acc, top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, validate the model we are examining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_model(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for showing an image/adversarial example and the correct/predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(img, label, pred, K=5, adv_img=None, noise=None):\n",
    "    \n",
    "    if isinstance(img, torch.Tensor):\n",
    "        # Tensor image to numpy\n",
    "        img = img.cpu().permute(1, 2, 0).numpy()\n",
    "        img = (img * NORM_STD[None,None]) + NORM_MEAN[None,None] # What effect does removing/changing this line have?\n",
    "        img = np.clip(img, a_min=0.0, a_max=1.0)\n",
    "        label = label.item()\n",
    "    \n",
    "    # Plot on the left the image with the true label as title.\n",
    "    # On the right, have a horizontal bar plot with the top k predictions including probabilities\n",
    "    if noise is None or adv_img is None:\n",
    "        _, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n",
    "    else:\n",
    "        _, ax = plt.subplots(1, 5, figsize=(12,2), gridspec_kw={'width_ratios': [1, 1, 1, 1, 2]})\n",
    "    \n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(label_names[label])\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    if adv_img is not None and noise is not None:\n",
    "        # Visualize adversarial images\n",
    "        adv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n",
    "        adv_img = (adv_img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
    "        adv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n",
    "        ax[1].imshow(adv_img)\n",
    "        ax[1].set_title('Adversarial')\n",
    "        ax[1].axis('off')\n",
    "        # Visualize noise\n",
    "        noise = noise.cpu().permute(1, 2, 0).numpy()\n",
    "        noise = noise * 0.5 + 0.5 # Scale between 0 to 1 \n",
    "        ax[2].imshow(noise)\n",
    "        ax[2].set_title('Noise')\n",
    "        ax[2].axis('off')\n",
    "        # buffer\n",
    "        ax[3].axis('off')\n",
    "    \n",
    "    if abs(pred.sum().item() - 1.0) > 1e-4:\n",
    "        pred = torch.softmax(pred, dim=-1)          # What shape is pred here?\n",
    "    topk_vals, topk_idx = pred.topk(K, dim=-1)\n",
    "    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
    "    ax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n",
    "    ax[-1].set_yticks(np.arange(K))\n",
    "    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n",
    "    ax[-1].invert_yaxis()\n",
    "    ax[-1].set_xlabel('Confidence')\n",
    "    ax[-1].set_title('Predictions')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exmp_batch, label_batch = next(iter(data_loader))\n",
    "with torch.no_grad():\n",
    "    preds = pretrained_model(exmp_batch.to(device))\n",
    "print(f\"len(exmp_batch): {len(exmp_batch)}\")\n",
    "for i in range(0,17,5):\n",
    "    show_prediction(exmp_batch[i], label_batch[i], preds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Gradient Sign Method (FGSM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_sign_method(model, imgs, labels, epsilon=0.2):\n",
    "    # Determine prediction of the model\n",
    "    inp_imgs = imgs.clone().requires_grad_()\n",
    "    preds = model(inp_imgs.to(device))\n",
    "    preds = F.log_softmax(preds, dim=-1)\n",
    "    # Calculate loss by NLL (what is NLL here -- negative lost likelihood?)\n",
    "    loss = -torch.gather(preds, 1, labels.to(device).unsqueeze(dim=-1))\n",
    "    loss.sum().backward()\n",
    "    # Update image to adversarial example\n",
    "    noise_grad = torch.sign(inp_imgs.grad.to(imgs.device)).detach()\n",
    "    fake_imgs = imgs + epsilon * noise_grad\n",
    "    fake_imgs.detach_()\n",
    "    return fake_imgs, noise_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using FGSM on example images.\n",
    "\n",
    "Observe that for `epsilon = 0.02`, the adversarial example results in classifications spread across a similar class of objects: different dog breeds for dogs, different fish breeds for fish.\n",
    "\n",
    "For `epsilon = 0.5` however, the results are in unrelated categories: bears are labeled as anything from a dog to a sock, cars are seen as jigsaw puzzles, peacocks, or chain link fences.\n",
    "\n",
    "In the `epsilon = 0.5` case, the images are noticeably blurred, but are still clearly an element of a larger category such as bear, car, shark, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_imgs, noise_grad = fast_gradient_sign_method(pretrained_model, exmp_batch, label_batch, epsilon=0.02)\n",
    "with torch.no_grad():\n",
    "    adv_preds = pretrained_model(adv_imgs.to(device))\n",
    "    \n",
    "for i in range(0,17,5):\n",
    "    show_prediction(exmp_batch[i], label_batch[i], adv_preds[i], adv_img=adv_imgs[i], noise=noise_grad[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the adversarial examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_model(data_loader, img_func=lambda x,y: fast_gradient_sign_method(pretrained_model,x,y, epsilon=0.02)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New content\n",
    "Up to now, this material has largely been taken from Philip Lippe's tutorial notebook.\n",
    "The question now is how to examine activations of neurons in adversarial examples.\n",
    "Try it out:\n",
    "Steps\n",
    "1. Learn how to view model activations \"neuron by neuron\" as they occur using PyTorch.\n",
    "2. Find a way to record activations in a way that is meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_label = next(iter(data_loader))\n",
    "test_img= test_img[1:2]\n",
    "test_label = test_label[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What shape is output from a model?\n",
    "with torch.no_grad():\n",
    "    preds = pretrained_model(test_img.to(device))\n",
    "    print(preds.shape) # preds has shape batch_size x # classes; so in this case, 1 x 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to convert a model's output to the model's most confident classification\n",
    "y = torch.argmax(preds[-1])\n",
    "print(y.item())\n",
    "adv = fast_gradient_sign_method(pretrained_model, test_img, test_label, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to an adversarial example\n",
    "adv_preds = pretrained_model(adv[0].to(device))\n",
    "show_prediction(test_img.reshape(3,224,224), test_label[0], adv_preds[0], adv_img=adv[0][0], noise=adv[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the activations of specific layers\n",
    "Use the feature extractor methods to get the activations at each layer and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of layers in this model\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "nodes, _= get_graph_node_names(pretrained_model)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature extractor for all the above nodes\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "feature_extractor = create_feature_extractor(pretrained_model, return_nodes=nodes)\n",
    "# `out` will be a dict of Tensors, each representing a feature map\n",
    "out = feature_extractor(test_img.to(device))\n",
    "out_adv = feature_extractor(adv[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for printing summary statistics \n",
    "def print_aggregations(feature, feature_name):\n",
    "    print(f\"{feature_name} max: {feature.max()}\")\n",
    "    print(f\"{feature_name} min: {feature.min()}\")\n",
    "    print(f\"{feature_name} mean: {feature.mean()}\")\n",
    "    print(f\"{feature_name} median: {feature.median()}\")\n",
    "    print(f\"{feature_name} std: {feature.std()}\")\n",
    "    print(f\"{feature_name} norm: {feature.norm()}\")\n",
    "    print(f\"{feature_name} len: {feature.shape}\")\n",
    "\n",
    "# Filter out 0 activations since these are a magic value due to how RELU truncates\n",
    "# Question: is this actually the right thing to do when considering activations?\n",
    "# Certainly, 0 values will skew the moments of the distribution of activation, e.g. the mean activation will be lower if 0 is included\n",
    "def filter_zero_activations(arr):\n",
    "    copy = arr[arr != 0]\n",
    "    return copy\n",
    "\n",
    "# Make a histogram plot of an activation, an adversarial activation, and the difference in activations between them\n",
    "# Activations with value 0 are filtered out from all three examples\n",
    "# The number of activations (y-axis) of a given strength are printed on a log scale for ease of viewing\n",
    "# The strength of the activation (or difference in activation) on the x-axis uses a linear scale.\n",
    "def hist_plot_activations(feature, feature_adv, feature_name=''):\n",
    "    diff = feature - feature_adv\n",
    "    fig, axs = plt.subplots(1, 3,figsize=(7,7),constrained_layout=True)\n",
    "    fig.suptitle(feature_name, fontsize='15')\n",
    "    axs[0].hist(filter_zero_activations(feature))\n",
    "    axs[0].set_yscale('log')\n",
    "    axs[0].set_title('Feature')\n",
    "    axs[1].hist(filter_zero_activations(feature_adv))\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[1].set_title('Adversarial')\n",
    "    axs[2].hist(filter_zero_activations(diff))\n",
    "    axs[2].set_yscale('log')\n",
    "    axs[2].set_title('Difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine `features.6` layer\n",
    "feature = out[\"features.6\"].detach().flatten()\n",
    "feature_adv = out_adv[\"features.6\"].detach().flatten()\n",
    "feature_diff = feature - feature_adv\n",
    "\n",
    "print_aggregations(feature, \"feature\")\n",
    "print_aggregations(filter_zero_activations(feature), \"filtered-feature\")\n",
    "print_aggregations(feature_diff, \"feature_diff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of the aggregation function\n",
    "hist_plot_activations(feature.cpu().numpy(), feature_adv.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting a feature\n",
    "def plot_feature(feature_name, feature_extractor):\n",
    "    feature = feature_extractor[feature_name].detach().flatten().cpu().numpy()\n",
    "    feature_adv = out_adv[feature_name].detach().flatten().cpu().numpy()\n",
    "    hist_plot_activations(feature, feature_adv, feature_name)\n",
    "    # Return activations, adversarial activations, and difference\n",
    "    return [feature, feature_adv, feature - feature_adv]\n",
    "\n",
    "# Plot all features in a model, up to some maximum number of layers (default of 5)\n",
    "def plot_all_features(start=0, end=5):\n",
    "    if start >= end:\n",
    "        print('Start index must be less than end index')\n",
    "    nodes, _= get_graph_node_names(pretrained_model)\n",
    "    accumulator = []\n",
    "    layers_to_plot = nodes[start:end]\n",
    "    for name in layers_to_plot:\n",
    "        activations = plot_feature(name, out)\n",
    "        accumulator.append(activations)\n",
    "                \n",
    "    return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the activations of the 2nd through 7th layers of the model\n",
    "_ = plot_all_features(1, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows where I started to learn more about Captum, a library for doing attributions.\n",
    "\n",
    "As you can see, I didn't get very far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# The question of whether and adversarial example is \"activating\" several neurons or working on a few\n",
    "# is a ticklish question; one way is to use conductance and oblations to determine which neurons have the strongest\n",
    "# impact on the result.\n",
    "# Start working on attributions\n",
    "from captum.attr import (\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    IntegratedGradients,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel,\n",
    ")\n",
    "ig = IntegratedGradients(pretrained_model)\n",
    "baseline = torch.zeros_like(test_img)\n",
    "attributions, delta = ig.attribute(test_img, baseline, target=0, return_convergence_delta=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
